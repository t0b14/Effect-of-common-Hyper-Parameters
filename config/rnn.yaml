experiment:
  title: Single-Layer RNN
  options:
    run_name: "new_layer"
    train_n_test: 1
    visualize: 1
    show_plot: 0
    use_wandb: 1
    make_weights_histograms: 1
    make_gradients_plot: 1
    make_hidden_state_plot: 1

  model:
    network_name: rnn
    in_dim: 4
    hidden_dims: 100
    out_dim: 1
    tau: 10.
    num_processes: 1
  
  training:
    dataset_name: ctxt
    n_epochs: 1000
    batch_size: 16
    total_seq_length: 1400
    seq_length: 100 # larger lengths won't fit well into cpu memory
    training_help: 0 # 0 to n_epochs (iterations)
    hidden_dims: 100
    n_trials: 5
    with_inputnoise: 0
    noise_level: 1 # number from 1 to 10; requires with_inputnoise; 0.15 * noiselevel
    coherency_intervals: "uniform" # "original" or "uniform"
    output_path: ./io/output/rnn1
    output_path_plots: ./io/output/rnn1/histograms_and_plots

  optimizer:
    optimizer_name: adagrad
    lr: 0.001
    betas: [0.9, 0.999]
    eps: 1.e-7
    lr_decay: 1.e-4
    weight_decay: 1.e-6
    amsgrad: false
    momentum: 0.9
    apply_gradient_clipping: 0

  seed: 42